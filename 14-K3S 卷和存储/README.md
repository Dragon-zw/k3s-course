Video Referenceï¼š[14-å·å’Œå­˜å‚¨](https://www.bilibili.com/video/BV19r4y1v7UT/?spm_id_from=333.788&vd_source=9560c118fae1db9638f05a6ba2527085)

GitHub README.mdï¼š[14-å·å’Œå­˜å‚¨](https://github.com/kingsd041/k3s-tutorial/tree/main/14-å·å’Œå­˜å‚¨)

# 0 K3s çš„å­˜å‚¨æœ‰ä»€ä¹ˆå˜åŒ–ï¼Ÿ

K3s åˆ é™¤äº†å‡ ä¸ªå¯é€‰çš„å·æ’ä»¶å’Œæ‰€æœ‰å†…ç½®çš„ï¼ˆæœ‰æ—¶è¢«ç§°ä¸º "æ ‘å†…"ï¼‰äº‘æä¾›å•†ã€‚æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†å®ç°æ›´å°çš„äºŒè¿›åˆ¶æ–‡ä»¶å¤§å°ï¼Œå¹¶é¿å…å¯¹ç¬¬ä¸‰æ–¹äº‘æˆ–æ•°æ®ä¸­å¿ƒæŠ€æœ¯å’ŒæœåŠ¡çš„ä¾èµ–ï¼Œè¿™äº›æŠ€æœ¯å’ŒæœåŠ¡åœ¨è®¸å¤š K3s çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­å¯èƒ½æ— æ³•ä½¿ç”¨ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥èƒ½å¤Ÿè¿™æ ·åšï¼Œæ˜¯å› ä¸ºåˆ é™¤è¿™äº›æ’ä»¶æ—¢ä¸å½±å“ Kubernetes çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¹Ÿä¸å½±å“ä¸€è‡´æ€§ã€‚

ä»¥ä¸‹æ˜¯å·²ç»ä» K3s ä¸­åˆ é™¤çš„å·æ’ä»¶ï¼š

- cephfs
- fc
- flocker
- git_repo
- glusterfs
- portworx
- quobyte
- rbd
- storageos

è¿™ä¸¤ä¸ªç»„ä»¶éƒ½æœ‰æ ‘å¤–çš„æ›¿ä»£å“ï¼Œå¯ä»¥ä¸ K3s ä¸€èµ·ä½¿ç”¨ï¼šKubernetes çš„[å®¹å™¨å­˜å‚¨æ¥å£ï¼ˆCSIï¼‰](https://github.com/container-storage-interface/spec/blob/master/spec.md)å’Œ[äº‘æä¾›å•†æ¥å£ï¼ˆCPIï¼‰](https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/)ã€‚

Kubernetes ç»´æŠ¤è€…æ­£åœ¨ç§¯æå°†æ ‘å†…å·æ’ä»¶è¿ç§»åˆ° CSI é©±åŠ¨ã€‚æœ‰å…³è¿™ä¸€è¿ç§»çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/)ã€‚

# 1 å·å’Œå­˜å‚¨

å½“éƒ¨ç½²ä¸€ä¸ªéœ€è¦ä¿ç•™æ•°æ®çš„åº”ç”¨ç¨‹åºæ—¶ï¼Œä½ éœ€è¦åˆ›å»ºæŒä¹…å­˜å‚¨ã€‚æŒä¹…å­˜å‚¨å…è®¸æ‚¨ä»è¿è¡Œåº”ç”¨ç¨‹åºçš„ pod å¤–éƒ¨å­˜å‚¨åº”ç”¨ç¨‹åºæ•°æ®ã€‚å³ä½¿åº”ç”¨ç¨‹åºçš„ pod å‘ç”Ÿæ•…éšœï¼Œè¿™ç§å­˜å‚¨æ–¹å¼ä¹Ÿå¯ä»¥ä½¿æ‚¨ç»´æŠ¤åº”ç”¨ç¨‹åºæ•°æ®ã€‚

æœ¬èŠ‚ä»‹ç»äº†å¦‚ä½•é€šè¿‡ `local storage provider` æˆ– `Longhorn` æ¥è®¾ç½®æŒä¹…å­˜å‚¨ã€‚

## 1.1 ğŸ’ è®¾ç½® Local Storage Provider

K3s è‡ªå¸¦ Rancher çš„ `Local Path Provisioner`ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿä½¿ç”¨å„è‡ªèŠ‚ç‚¹ä¸Šçš„æœ¬åœ°å­˜å‚¨æ¥å¼€ç®±å³ç”¨åœ°åˆ›å»º pvcã€‚

Local Path Provisioner ä¸º Kubernetes/K3s ç”¨æˆ·æä¾›äº†ä¸€ç§åˆ©ç”¨æ¯ä¸ªèŠ‚ç‚¹ä¸­çš„æœ¬åœ°å­˜å‚¨çš„æ–¹æ³•ã€‚æ ¹æ®ç”¨æˆ·é…ç½®ï¼ŒLocal Path Provisioner å°†è‡ªåŠ¨åœ¨èŠ‚ç‚¹ä¸Šåˆ›å»ºåŸºäº `hostPath` çš„æŒä¹…å·ã€‚å®ƒåˆ©ç”¨äº† `Kubernetes Local Persistent Volume` ç‰¹æ€§å¼•å…¥çš„ç‰¹æ€§ï¼Œä½†ä»–æ¯” Kubernetes ä¸­å†…ç½®çš„ local pv ç‰¹æ€§æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚

èŒƒä¾‹ï¼šéƒ¨ç½²æœ¬åœ°ç¯å¢ƒ K3s é›†ç¾¤

```shell
# å®‰è£…Dockerå®¹å™¨è¿è¡Œæ—¶ç¯å¢ƒ
# åœ¨ K3s èŠ‚ç‚¹ä¸Šå®‰è£… Dockerã€‚å¯ä»¥ä½¿ç”¨ Rancher çš„ä¸€ä¸ªDocker å®‰è£…è„šæœ¬æ¥å®‰è£… Dockerï¼š
$ curl https://releases.rancher.com/install-docker/19.03.sh | sh

# é…ç½®é•œåƒåŠ é€Ÿå™¨
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://po13h3y1.mirror.aliyuncs.com","http://hub-mirror.c.163.com","https://mirror.ccs.tencentyun.com","http://f1361db2.m.daocloud.io"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker && sudo systemctl enable docker
# K3s Master
$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn \
  INSTALL_K3S_VERSION="v1.21.14+k3s1" \
  INSTALL_K3S_EXEC="server --docker" \
  sh -s -
$ cat /var/lib/rancher/k3s/server/token
K1046b15950a113b87d32b8d245b15490b76722b81f16b79f0e1eb912d2fe0d6b9a::server:e8f98075887822efc50aed139e778e2d

# K3s Worker
$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn \
  K3S_URL=https://10.0.0.51:6443 \
  K3S_TOKEN=e8f98075887822efc50aed139e778e2d INSTALL_K3S_VERSION="v1.21.14+k3s1" \
  INSTALL_K3S_EXEC="--node-ip=10.0.0.52 --docker" \
  sh -

$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn \
  K3S_URL=https://10.0.0.51:6443 \
  K3S_TOKEN=e8f98075887822efc50aed139e778e2d INSTALL_K3S_VERSION="v1.21.14+k3s1" \
  INSTALL_K3S_EXEC="--node-ip=10.0.0.53 --docker" \
  sh -
```

èŒƒä¾‹ï¼šæœ¬åœ°ç¯å¢ƒæƒ…å†µ

```shell
# æœ¬åœ°çš„ç¯å¢ƒ
$ kubectl get nodes -o wide
NAME   STATUS   ROLES                  AGE     VERSION         INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
k3s1   Ready    control-plane,master   5m24s   v1.21.14+k3s1   10.0.0.51     <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.15
k3s2   Ready    <none>                 2m40s   v1.21.14+k3s1   10.0.0.52     <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.15
k3s3   Ready    <none>                 39s     v1.21.14+k3s1   10.0.0.53     <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.15

# å¯ä»¥æŸ¥çœ‹åˆ°ç›¸åº”çš„Podèµ„æº
$ kubectl get pods -A
NAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE
kube-system   coredns-574bcc6c46-9lgrb                  1/1     Running     0          5m21s
kube-system   helm-install-traefik-crd-wjxx7            0/1     Completed   0          5m22s
kube-system   helm-install-traefik-529q6                0/1     Completed   1          5m22s
# local-path-provisioner é»˜è®¤çš„éƒ¨ç½²é…ç½®æ–‡ä»¶åœ¨/var/lib/rancher/k3s/server/manifests/local-storage.yaml
kube-system   local-path-provisioner-84bb864455-r8pcb   1/1     Running     0          5m21s
kube-system   metrics-server-ff9dbcb6c-b2l6s            1/1     Running     0          5m21s
kube-system   traefik-56c4b88c4b-86tw2                  1/1     Running     0          4m14s
kube-system   svclb-traefik-kwzfl                       2/2     Running     0          4m14s
kube-system   svclb-traefik-6xfr5                       2/2     Running     0          2m42s
kube-system   svclb-traefik-sqm49                       2/2     Running     0          42s

# K3s Master æŸ¥çœ‹ç›¸åº”èµ„æºé…ç½®æ–‡ä»¶
$ ls /var/lib/rancher/k3s/server/manifests
ccm.yaml  c_dns.yaml  local-storage.yaml  metrics-server  rolebindings.yaml  traefik.yaml
# æŸ¥çœ‹ç›¸åº”çš„StorageClassçš„èµ„æºæ–‡ä»¶
$ cat /var/lib/rancher/k3s/server/manifests/local-storage.yaml
$ kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  9m18s
$ cat /var/lib/rancher/k3s/server/manifests/local-storage.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-path-provisioner-service-account
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: local-path-provisioner-role
rules:
- apiGroups: [""]
  resources: ["nodes", "persistentvolumeclaims", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["endpoints", "persistentvolumes", "pods"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "patch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: local-path-provisioner-bind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: local-path-provisioner-role
subjects:
- kind: ServiceAccount
  name: local-path-provisioner-service-account
  namespace: kube-system
---
# éƒ¨ç½²Deploymentçš„èµ„æº
apiVersion: apps/v1
kind: Deployment
metadata:
  name: local-path-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: local-path-provisioner
  template:
    metadata:
      labels:
        app: local-path-provisioner
    spec:
      priorityClassName: "system-node-critical"
      serviceAccountName: local-path-provisioner-service-account
      tolerations:
          - key: "CriticalAddonsOnly"
            operator: "Exists"
          - key: "node-role.kubernetes.io/control-plane"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "node-role.kubernetes.io/master"
            operator: "Exists"
            effect: "NoSchedule"
      containers:
      - name: local-path-provisioner
        image: rancher/local-path-provisioner:v0.0.21
        imagePullPolicy: IfNotPresent
        command:
        - local-path-provisioner
        - start
        - --config
        - /etc/config/config.json
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config/
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      volumes:
        - name: config-volume
          configMap:
            name: local-path-config
---
# åˆ›å»ºStorageClassçš„å­˜å‚¨å¼•æ“
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# åˆ›å»ºStorageClassçš„é…ç½®
kind: ConfigMap
apiVersion: v1
metadata:
  name: local-path-config
  namespace: kube-system
data:
  config.json: |-
    {
      "nodePathMap":[
      {
        "node":"DEFAULT_PATH_FOR_NON_LISTED_NODES",
        "paths":["/var/lib/rancher/k3s/storage"] # é»˜è®¤å­˜å‚¨ç›®å½•ï¼ŒVolumeæ•°æ®å­˜å‚¨çš„ä½ç½®
        # ç®¡ç†å‘˜å¯ä»¥è‡ªå®šä¹‰å­˜å‚¨ç›®å½•è·¯å¾„
      }
      ]
    }
  setup: |-
    #!/bin/sh
    while getopts "m:s:p:" opt
    do
        case $opt in
            p)
            absolutePath=$OPTARG
            ;;
            s)
            sizeInBytes=$OPTARG
            ;;
            m)
            volMode=$OPTARG
            ;;
        esac
    done
    mkdir -m 0777 -p ${absolutePath}
    chmod 701 ${absolutePath}/..
  teardown: |-
    #!/bin/sh
    while getopts "m:s:p:" opt
    do
        case $opt in
            p)
            absolutePath=$OPTARG
            ;;
            s)
            sizeInBytes=$OPTARG
            ;;
            m)
            volMode=$OPTARG
            ;;
        esac
    done
    rm -rf ${absolutePath}
  helperPod.yaml: |-
    apiVersion: v1
    kind: Pod
    metadata:
      name: helper-pod
    spec:
      containers:
      - name: helper-pod
        image: rancher/mirrored-library-busybox:1.34.1
```

### 1.1.1 pvc.yaml

```yaml
cat <<EOF > pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-path-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 2Gi
EOF
```

### 1.1.2 pod.yaml

```yaml
cat <<EOF > pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: local-path-pvc
EOF
```

### 1.1.3 åº”ç”¨ yaml:

```shell
$ kubectl create -f pvc.yaml
persistentvolumeclaim/local-path-pvc created

$ kubectl create -f pod.yaml
pod/volume-test created
```

### 1.1.4 ç¡®è®¤ PV å’Œ PVC å·²åˆ›å»ºï¼š

```shell
# æŸ¥çœ‹PVå’ŒPVCçš„çŠ¶æ€
$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
persistentvolume/pvc-e33e16b1-94e4-40f1-b749-f2895cd5b0a6   2Gi        RWO            Delete           Bound    default/local-path-pvc   local-path              59s

NAME                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/local-path-pvc   Bound    pvc-e33e16b1-94e4-40f1-b749-f2895cd5b0a6   2Gi        RWO            local-path     3m8s

# æŸ¥çœ‹Podçš„è¿è¡ŒçŠ¶æ€
$ kubectl get pod -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE   NOMINATED NODE   READINESS GATES
volume-test   1/1     Running   0          93s   10.42.1.4   k3s2   <none>           <none>

# åœ¨Podä¸­ç¼–å†™æ–‡ä»¶ï¼Œå¹¶è¿›è¡ŒæŸ¥çœ‹Podå†…çš„æ–‡ä»¶å†…å®¹
$ kubectl exec -it volume-test -- sh -c "echo $(date) > /data/test.txt"
$ kubectl exec -it volume-test -- sh -c "cat /data/test.txt"
Thu Mar 23 23:58:59 CST 2023

# å¯ä»¥åˆ°å¯¹åº”çš„èŠ‚ç‚¹(k3s2)æŸ¥çœ‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
$ cd /var/lib/rancher/k3s/storage/
$ ls
pvc-c637caa5-fc94-4751-81c8-545141052691_default_local-path-pvc
$ cat pvc-c637caa5-fc94-4751-81c8-545141052691_default_local-path-pvc/test.txt
Fri Mar 23 23:58:59 CST 2023
```

èŒƒä¾‹ï¼šæ¨¡æ‹Ÿ Pod çš„æ•…éšœ

```shell
# æ¨¡æ‹ŸPodå‘ç”Ÿæ•…éšœ
$ kubectl delete -f pod.yaml
pod "volume-test" deleted
$ kubectl create -f pod.yaml
pod/volume-test created
# å‘ç°æ•°æ®ä¾æ—§å­˜åœ¨
$ kubectl exec -it volume-test -- sh -c "cat /data/test.txt"
Thu Mar 23 23:58:59 CST 2023
# æ¸…ç†æ•°æ®
$ kubectl delete -f pod.yaml -f pvc.yaml
```

## 1.2 ğŸ’ è®¾ç½® Longhorn

K3s æ”¯æŒ [Longhorn](https://github.com/longhorn/longhorn). Longhorn æ˜¯ Kubernetes çš„ä¸€ä¸ª`å¼€æºåˆ†å¸ƒå¼å—å­˜å‚¨ç³»ç»Ÿ`ã€‚

ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://github.com/longhorn/longhorn/blob/master/README.md)ã€‚

**Longhorn** æ˜¯ä¸€ä¸ªè½»é‡çº§ã€å¯é ã€åŠŸèƒ½å¼ºå¤§çš„ Kubernetes åˆ†å¸ƒå¼[å—å­˜å‚¨ç³»ç»Ÿ](https://cloudacademy.com/blog/object-storage-block-storage/)ã€‚

Longhorn ä½¿ç”¨å®¹å™¨å’Œå¾®æœåŠ¡å®ç°åˆ†å¸ƒå¼å—å­˜å‚¨ã€‚Longhorn ä¸ºæ¯ä¸ªå—è®¾å¤‡å·åˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„å­˜å‚¨æ§åˆ¶å™¨ï¼Œå¹¶åœ¨å­˜å‚¨åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šçš„å¤šä¸ªå‰¯æœ¬ä¹‹é—´åŒæ­¥å¤åˆ¶è¯¥å·ã€‚å­˜å‚¨æ§åˆ¶å™¨å’Œå‰¯æœ¬æœ¬èº«æ˜¯ä½¿ç”¨ Kubernetes ç¼–æ’çš„ã€‚

**ç‰¹å¾**

- ä¼ä¸šçº§åˆ†å¸ƒå¼å—å­˜å‚¨ï¼Œæ— å•ç‚¹æ•…éšœ
- å—å­˜å‚¨çš„å¢é‡å¿«ç…§
- å¤‡ä»½åˆ°åŸºäºé«˜æ•ˆæ›´æ”¹å—æ£€æµ‹æ„å»ºçš„è¾…åŠ©å­˜å‚¨ï¼ˆ[NFS](https://www.extrahop.com/resources/protocols/nfs/) æˆ– [S3](https://aws.amazon.com/s3/) å…¼å®¹å¯¹è±¡å­˜å‚¨ï¼‰
- å®šæœŸå¿«ç…§å’Œå¤‡ä»½
- è‡ªåŠ¨åŒ–ã€æ— ä¸­æ–­å‡çº§ã€‚æ‚¨å¯ä»¥å‡çº§æ•´ä¸ª Longhorn è½¯ä»¶å †æ ˆï¼Œè€Œä¸ä¼šä¸­æ–­æ­£åœ¨è¿è¡Œçš„å­˜å‚¨å·ã€‚
- ç›´è§‚çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ä»ªè¡¨æ¿

![img](assets/1679597256201-7863e78a-6d92-4526-b903-4275024e4fba.svg+xml)

### 1.2.1 å®‰è£… Longhornï¼š

```shell
# kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml

# å¯ä»¥ä¸‹è½½ longhorn.yaml é…ç½®æ–‡ä»¶
$ wget https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
$ kubectl create -f longhorn.yaml
```

Longhorn å°†è¢«å®‰è£…åœ¨å‘½åç©ºé—´ `longhorn-system` ä¸­ã€‚

```shell
# æŸ¥çœ‹longhorn-systemçš„å‘½åç©ºé—´çš„æ‰€æœ‰èµ„æº
# éœ€è¦æ‰€æœ‰çš„Podéƒ½å¯åŠ¨æˆåŠŸï¼Œlonghorn æ‰èƒ½å¯åŠ¨æˆåŠŸ
$ kubectl get all -n longhorn-system
NAME                                                      READY   STATUS    RESTARTS   AGE
pod/longhorn-recovery-backend-997bd57f-s4g7m              1/1     Running   0          12m
pod/longhorn-conversion-webhook-747fc8cbcd-bnnj6          1/1     Running   0          12m
pod/longhorn-ui-d996774-jrhq9                             1/1     Running   0          12m
pod/longhorn-recovery-backend-997bd57f-ncxlh              1/1     Running   0          12m
pod/longhorn-ui-d996774-s5dl9                             1/1     Running   0          12m
pod/longhorn-admission-webhook-54fc8fd679-sshbh           1/1     Running   0          12m
pod/longhorn-manager-cft7r                                1/1     Running   1          12m
pod/longhorn-manager-psrl8                                1/1     Running   0          12m
pod/longhorn-conversion-webhook-747fc8cbcd-bpkxq          1/1     Running   0          12m
pod/longhorn-manager-d9j8g                                1/1     Running   0          12m
pod/longhorn-driver-deployer-b89675844-j2tl9              1/1     Running   0          12m
pod/longhorn-admission-webhook-54fc8fd679-ndqg2           1/1     Running   0          12m
pod/engine-image-ei-b907910b-l7hb5                        1/1     Running   0          11m
pod/engine-image-ei-b907910b-fdswq                        1/1     Running   0          11m
pod/engine-image-ei-b907910b-hmw89                        1/1     Running   0          11m
pod/instance-manager-e-c11b800b4e57eae7de58b50ab934a0b2   1/1     Running   0          11m
pod/instance-manager-r-c11b800b4e57eae7de58b50ab934a0b2   1/1     Running   0          11m
pod/instance-manager-e-710ec2bc8a7d05f2c3e252558c8fb457   1/1     Running   0          11m
pod/instance-manager-r-710ec2bc8a7d05f2c3e252558c8fb457   1/1     Running   0          11m
pod/csi-attacher-84b96d64c8-npz8v                         1/1     Running   0          10m
pod/instance-manager-e-6b1036c0f406949531e101186b5a3e95   1/1     Running   0          11m
pod/csi-attacher-84b96d64c8-hhr8t                         1/1     Running   0          10m
pod/instance-manager-r-6b1036c0f406949531e101186b5a3e95   1/1     Running   0          11m
pod/csi-provisioner-6ccbfbf86f-flwvt                      1/1     Running   0          10m
pod/csi-resizer-58c959486d-m28n4                          1/1     Running   0          10m
pod/csi-resizer-58c959486d-zk7pq                          1/1     Running   0          10m
pod/csi-attacher-84b96d64c8-7q6dt                         1/1     Running   0          10m
pod/csi-snapshotter-7f744fbb67-jkw7k                      1/1     Running   0          10m
pod/csi-provisioner-6ccbfbf86f-qpn6x                      1/1     Running   0          10m
pod/csi-provisioner-6ccbfbf86f-6q528                      1/1     Running   0          10m
pod/longhorn-csi-plugin-2bg7h                             3/3     Running   0          10m
pod/csi-snapshotter-7f744fbb67-kpcsn                      1/1     Running   0          10m
pod/csi-resizer-58c959486d-f98ts                          1/1     Running   0          10m
pod/csi-snapshotter-7f744fbb67-xqdl9                      1/1     Running   0          10m
pod/longhorn-csi-plugin-r9jhn                             3/3     Running   0          10m
pod/longhorn-csi-plugin-xbls4                             3/3     Running   0          10m

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/longhorn-backend              ClusterIP   10.43.142.188   <none>        9500/TCP       12m
service/longhorn-conversion-webhook   ClusterIP   10.43.31.156    <none>        9443/TCP       12m
service/longhorn-admission-webhook    ClusterIP   10.43.205.196   <none>        9443/TCP       12m
service/longhorn-recovery-backend     ClusterIP   10.43.91.250    <none>        9600/TCP       12m
service/longhorn-engine-manager       ClusterIP   None            <none>        <none>         12m
service/longhorn-replica-manager      ClusterIP   None            <none>        <none>         12m
service/csi-attacher                  ClusterIP   10.43.143.109   <none>        12345/TCP      10m
service/csi-provisioner               ClusterIP   10.43.12.35     <none>        12345/TCP      10m
service/csi-resizer                   ClusterIP   10.43.116.184   <none>        12345/TCP      10m
service/csi-snapshotter               ClusterIP   10.43.174.255   <none>        12345/TCP      10m
service/longhorn-frontend             ClusterIP   10.43.151.184   <none>        80/TCP         12m

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/longhorn-manager           3         3         3       3            3           <none>          12m
daemonset.apps/engine-image-ei-b907910b   3         3         3       3            3           <none>          11m
daemonset.apps/longhorn-csi-plugin        3         3         3       3            3           <none>          10m

NAME                                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/longhorn-recovery-backend     2/2     2            2           12m
deployment.apps/longhorn-ui                   2/2     2            2           12m
deployment.apps/longhorn-conversion-webhook   2/2     2            2           12m
deployment.apps/longhorn-driver-deployer      1/1     1            1           12m
deployment.apps/longhorn-admission-webhook    2/2     2            2           12m
deployment.apps/csi-attacher                  3/3     3            3           10m
deployment.apps/csi-provisioner               3/3     3            3           10m
deployment.apps/csi-resizer                   3/3     3            3           10m
deployment.apps/csi-snapshotter               3/3     3            3           10m

NAME                                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/longhorn-recovery-backend-997bd57f       2         2         2       12m
replicaset.apps/longhorn-ui-d996774                      2         2         2       12m
replicaset.apps/longhorn-conversion-webhook-747fc8cbcd   2         2         2       12m
replicaset.apps/longhorn-driver-deployer-b89675844       1         1         1       12m
replicaset.apps/longhorn-admission-webhook-54fc8fd679    2         2         2       12m
replicaset.apps/csi-attacher-84b96d64c8                  3         3         3       10m
replicaset.apps/csi-provisioner-6ccbfbf86f               3         3         3       10m
replicaset.apps/csi-resizer-58c959486d                   3         3         3       10m
replicaset.apps/csi-snapshotter-7f744fbb67               3         3         3       10m

# å°†service/longhorn-frontendä¿®æ”¹ä¸ºNodePortçš„æ–¹å¼
$ kubectl edit svc -n longhorn-system longhorn-frontend
# type: ClusterIP -> type: NodePort

# å¯ä»¥ç›´æ¥è¿›è¡Œä¿®æ”¹
$ kubectl patch svc longhorn-frontend -n longhorn-system -p '{"spec":{"type": "NodePort"}}'
service/longhorn-frontend patched

# æŸ¥çœ‹Serviceçš„é…ç½®
$ kubectl get svc -n longhorn-system longhorn-frontend
NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
longhorn-frontend   NodePort   10.43.151.184   <none>        80:30193/TCP   13m

# ä¼šå‘ç°åˆ›å»ºä¸€ä¸ªlonghornçš„StorageClasså­˜å‚¨ç±»
$ kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  28m
longhorn (default)     driver.longhorn.io      Delete          Immediate              true                   16m
```

é€šè¿‡æµè§ˆå™¨è®¿é—®`http://NodeIP:[longhorn-frontend-NodePort]`çš„åŸŸåï¼Œå°±å¯ä»¥ longhorn çš„ WebUIã€‚

![img](assets/1679597040508-75b42451-bc76-4c04-baa3-271257296e51.png)

![img](assets/1679597136897-54a3f3e6-196e-4a8f-8ddb-ed4768bee9ac.png)

### 1.2.2 åˆ›å»º PVC å’Œ podï¼š

#### 1.2.2.1 longhorn-pvc.yaml

```yaml
cat <<EOF > longhorn-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 2Gi
EOF
```

#### 1.2.2.2 longhorn-pod.yaml

```yaml
cat <<EOF > longhorn-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: default
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF
```

#### 1.2.2.3 ç¡®è®¤ PV å’Œ PVC å·²åˆ›å»ºï¼š

```shell
$ kubectl create -f longhorn-pvc.yaml -f longhorn-pod.yaml
persistentvolumeclaim/longhorn-volv-pvc created
pod/volume-test created

$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE
persistentvolume/pvc-8934d1d6-1f4e-48ee-8f81-6024df5bf4ef   2Gi        RWO            Delete           Bound    default/longhorn-volv-pvc   longhorn                102s

NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/longhorn-volv-pvc   Bound    pvc-8934d1d6-1f4e-48ee-8f81-6024df5bf4ef   2Gi        RWO            longhorn       104s
```

å¯ä»¥é€šè¿‡ longhorn çš„ WebUI æŸ¥çœ‹ Volume çš„èµ„æº

![img](assets/1679597347456-3085cc0c-df5a-438e-a1b5-d0311bad1f69.png)

![img](assets/1679597390749-819e7ebe-0cea-4358-b4f7-7745a0ba1c8d.png)

## 1.3 ğŸ’ è®¾ç½® NFS

å¦‚æœä½ çš„ K3S é›†ç¾¤æ˜¯ v1.20+ï¼Œåœ¨ nfs provisioner åˆ›å»º PersistentVolumeClaimï¼ŒPersistentVolumeClaim ä¿æŒ Pending çŠ¶æ€, ä¸” nfs provisioner ä¼šæŠ¥é”™ï¼š

```shell
I0512 03:01:54.863533       1 controller.go:926] provision "default/v1" class "nfs-provisioner": started
E0512 03:01:54.867892       1 controller.go:943] provision "default/v1" class "nfs-provisioner": unexpected error getting claim reference: selfLink was empty, can't make reference
```

### 1.3.1 åŸå› 

åœ¨ k8s 1.20 ä¸­ï¼Œå·²æ ¹æ® [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md) åˆ é™¤ `selfLink` å‚æ•°ã€‚

æ­£å¦‚ [github è¯„è®º](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/issues/25#issuecomment-742616668) ä¸­æ‰€æŒ‡å‡ºçš„ï¼Œå°† `RemoveSelfLink` è®¾ç½®ä¸º `false` å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

- ä¿®æ”¹ k3s çš„é…ç½®æ–‡ä»¶

![img](assets/1679597525633-f53d6ad4-15d7-4a8e-ba0d-430297258415.png)

- ä¿®æ”¹ç›¸åº”çš„é•œåƒ

![img](assets/1679597563258-b2eca44b-7ff2-42c5-af20-eb84660fee32.png)

### 1.3.2 è§£å†³æ–¹æ³•

```shell
$ curl -sfL https://get.k3s.io | sh -s - --kube-apiserver-arg "feature-gates=RemoveSelfLink=false"
```

æˆ–ä¿®æ”¹`k3s.service`çš„æœåŠ¡é…ç½®æ–‡ä»¶

```shell
$ cat /etc/systemd/system/k3s.service
...
ExecStart=/usr/local/bin/k3s \
    server \
        '--kube-apiserver-arg' \
        'feature-gates=RemoveSelfLink=false' \
...
```

æœ€åï¼Œä¸è¦å¿˜è®°é‡å¯ K3s è§¦å‘æ›´æ–°ã€‚
